{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nafta.\n",
      "i loves than part of it.\n",
      "but i feel good such nice to buy than ever to pay a big excavators, go again – forget, who are all-time, it’s right.\n",
      "and important, i guess around the way, it to this is get 2 million.\n",
      "but...i called local leading in the middle class is out and they’re doing.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "class Generator:\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, sentences, ngram = 2):\n",
    "      \n",
    "        self.langmod = {} \n",
    "        self.ngram = ngram\n",
    "        words = self._clean_sentences(sentences)\n",
    "   \n",
    "   \n",
    "        self.langmod[1] = self._bigram_mle_model(words)\n",
    "        for i in range(2, ngram + 1):\n",
    "            self.langmod[i] = self._ngram_mle_model(words, i + 1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _unigram_mle_model(words):\n",
    "        ngrams = nltk.ngrams(words, 1)\n",
    "        cfdist = nltk.ConditionalFreqDist((tuple(x[:(0)]), x[0]) for x in \\\n",
    "                ngrams)\n",
    "        return nltk.ConditionalProbDist(cfdist, nltk.MLEProbDist)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _bigram_mle_model(words):\n",
    "        cfdist = nltk.ConditionalFreqDist(nltk.bigrams(words))\n",
    "        return nltk.ConditionalProbDist(cfdist, nltk.MLEProbDist)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ngram_mle_model(words, n):\n",
    "        ngrams = nltk.ngrams(words, n)\n",
    "        cfdist = nltk.ConditionalFreqDist((tuple(x[:(n - 1)]), x[n - 1]) for x in \\\n",
    "                ngrams)\n",
    "        return nltk.ConditionalProbDist(cfdist, nltk.MLEProbDist)\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_sentences(sents):\n",
    "       \n",
    "        result = []\n",
    "        for sent in sents:\n",
    "            result.append(Generator.begin)\n",
    "            result.extend([word for word in sent if word not in \\\n",
    "                Generator.IGNORED])\n",
    "            result.append(Generator.end)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def formation(sentence):\n",
    "    \n",
    "        result = []\n",
    "        buf = \"\"\n",
    "        for word in sentence:\n",
    "            if word in Generator.NO_SPACE_AFTER:\n",
    "                buf += word\n",
    "            elif word in Generator.NO_SPACE_BEFORE or word[0] in \\\n",
    "                    Generator.NO_SPACE_BEFORE_PREFIX:\n",
    "                if len(result) == 0:\n",
    "                    result.append(buf + word)\n",
    "                else:\n",
    "                    result[-1] += buf + word\n",
    "                buf = \"\"\n",
    "            else:\n",
    "                result.append(buf + word)\n",
    "                buf = \"\"\n",
    "        return \"\".join(result)\n",
    "\n",
    "    def generate(self, as_list = False):\n",
    "\n",
    "        sentence = []\n",
    "        context = [Generator.begin]\n",
    "        while context[-1] != Generator.end:\n",
    "        \n",
    "            if len(context) == 1:\n",
    "                cur = self.langmod[1][context[0]].generate()\n",
    "                context.append(cur)\n",
    "            else:\n",
    "                cur = self.langmod[len(context)][tuple(context)].generate()\n",
    "                context.append(cur)\n",
    "                if len(context) >= self.ngram:\n",
    "                    context.pop(0)\n",
    "\n",
    "            if cur != Generator.end:\n",
    "                sentence.append(cur)\n",
    "\n",
    "        if as_list:\n",
    "            return sentence\n",
    "        else:\n",
    "            return self.formation(sentence)\n",
    "\n",
    "    IGNORED = ['\"', '\\'']\n",
    "    \n",
    "    NO_SPACE_BEFORE = [',', '.', '?', ':', ';', ')', '!', \"n't\", \"''\", \"'t\"]\n",
    "    NO_SPACE_BEFORE_PREFIX = ['.', '\\'']\n",
    "    NO_SPACE_AFTER = ['(', '``']\n",
    "    begin = '<s>'\n",
    "    end = '</s>'\n",
    "\n",
    "\n",
    "def read_sentences_from_file(file_path):\n",
    "        fileObj = open(file_path, 'r',encoding=\"utf8\")\n",
    "        text = fileObj.read()\n",
    "        tokens = nltk.sent_tokenize(text.lower())\n",
    "        return tokens\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokens = read_sentences_from_file(\"speeches.txt\")\n",
    "\n",
    "    obj=Generator(tokens,6)\n",
    " \n",
    "    for i in range(1,6):\n",
    "        print(obj.generate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
